{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Named Entity Recognition Case Study\n",
    "\n",
    "### About\n",
    "Twitter is a microblogging and social networking service on which users post and interact with messages known as \"tweets\". Every second, on average, around 6,000 tweets are tweeted on Twitter, corresponding to over 350,000 tweets sent per minute, 500 million tweets per day.\n",
    "\n",
    "### Problem statement \n",
    "Twitter wants to automatically tag and analyze tweets for better understanding of the trends and topics without being dependent on the hashtags that the users use. Many users do not use hashtags or sometimes use wrong or mis-spelled tags, so they want to completely remove this problem and create a system of recognizing important content of the tweets.\n",
    "\n",
    "### Objective\n",
    "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities.\n",
    "We need to train models that will be able to identify the various named entities.\n",
    "\n",
    "### Data\n",
    "Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing in Text file format.\n",
    "The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "data_path = os.path.join(root_path,'data')\n",
    "train_data_path = os.path.join(data_path,'wnut 16.txt.conll')\n",
    "test_data_path = os.path.join(data_path,'wnut 16test.txt.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training file\n",
    "with open(train_data_path,'r') as f:\n",
    "    train_raw = f.read()\n",
    "with open(test_data_path,'r') as f:\n",
    "    test_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "def extract_ner_from_conll(conll_data):\n",
    "    # Split the data into sentences based on empty lines\n",
    "    sentences = [sentence.strip() for sentence in conll_data.strip().split('\\n\\n')]\n",
    "    ner_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = []\n",
    "        for token_entity in sentence.split('\\n'):\n",
    "            token, entity = token_entity.split('\\t')\n",
    "            tokenised_sentence.append((token,entity))\n",
    "        ner_data.append(tokenised_sentence)\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the raw files\n",
    "train_data = extract_ner_from_conll(train_raw)\n",
    "test_data = extract_ner_from_conll(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SammieLynnsMom', 'O'), ('@tg10781', 'O'), ('they', 'O'), ('will', 'O'), ('be', 'O'), ('all', 'O'), ('done', 'O'), ('by', 'O'), ('Sunday', 'O'), ('trust', 'O'), ('me', 'O'), ('*wink*', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# checking sentences after preprocessing\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data (including padding and OOV token) = 10588\n",
      "Maximum sentence length = 39\n",
      "Minimum sentence length = 1\n"
     ]
    }
   ],
   "source": [
    "# number of words in the vocabulary and lenght of sentences in the training data\n",
    "\n",
    "sentence_lenghts = list()\n",
    "word_set = set()\n",
    "for sentence in train_data:\n",
    "    sentence_lenghts.append(len(sentence))\n",
    "    for word in sentence:\n",
    "        word_set.add(word[0])\n",
    "        \n",
    "NUM_WORDS = len(word_set)+2 # +2 to include padding and out of vocabulary\n",
    "print(f\"Number of unique words in training data (including padding and OOV token) = {NUM_WORDS}\")\n",
    "print(f\"Maximum sentence length = {max(sentence_lenghts)}\")\n",
    "print(f\"Minimum sentence length = {min(sentence_lenghts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the max sentence length if 39, we will take a length of 45 in our model to incorporate for edge cases in inference\n",
    "SENTENCE_LENGTH = 45\n",
    "# we will keep the embedding dimensions to be 50 since the number of datapoints is small\n",
    "EMBEDDING_DIMS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities in training data = 21\n"
     ]
    }
   ],
   "source": [
    "# number of entities\n",
    "entity_set = set()\n",
    "for sentence in train_data:\n",
    "    for word in sentence:\n",
    "        entity_set.add(word[1])\n",
    "        \n",
    "NUM_ENTITIES = len(entity_set)\n",
    "print(f\"Number of unique entities in training data = {NUM_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to prepare the data to be fed into the model\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(\"[^A-Za-z0-9]+\",'',str(text).lower())\n",
    "\n",
    "def prepare_data(text_data):\n",
    "    \n",
    "    # initialize empty lists for sentences and entities\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in text_data:\n",
    "        \n",
    "        # initialize empty lists for sentence text and corresponding entities\n",
    "        word_list = []\n",
    "        entity_list = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            word = token[0]\n",
    "            entity = token[1]\n",
    "            if word in punctuations:\n",
    "                continue\n",
    "            else:\n",
    "                word_list.append(clean_text(word))\n",
    "                entity_list.append(entity)\n",
    "        \n",
    "        sentences.append(word_list)\n",
    "        entities.append(entity_list)\n",
    "    \n",
    "    # create a single string for each sentence and entity by joining elements with whitespace\n",
    "    sentences = np.array([' '.join(sentence) for sentence in sentences])\n",
    "    entities = np.array([' '.join(entity) for entity in entities])\n",
    "    \n",
    "    return (sentences,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the datapoints in train file is very low, we will merge the datasets and prepare our own train and test data\n",
    "data = []\n",
    "for i in train_data:\n",
    "    data.append(i)\n",
    "for i in test_data:\n",
    "    data.append(i)\n",
    "\n",
    "xdata, ydata = prepare_data(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion\n",
      "@SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*\n",
      "\n",
      "After conversion\n",
      "sammielynnsmom tg10781 they will be all done by sunday trust me wink\n",
      "\n",
      "Entities\n",
      "O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# checking sentences are conversion\n",
    "print(f\"Before conversion\\n{' '.join([word[0] for word in data[0]])}\\n\")\n",
    "print(f\"After conversion\\n{xdata[0]}\\n\")\n",
    "print(f\"Entities\\n{ydata[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the vectorizer layer before modeling\n",
    "sentence_tokenizer = TextVectorization(max_tokens=NUM_WORDS, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='sentence_tokenizer')\n",
    "sentence_tokenizer.adapt(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build and compile model\n",
    "def build_lstm_model(name='bi-lstm+crf'):\n",
    "    \n",
    "    # input layer for getting sentences\n",
    "    sentence_input = Input(shape=(1,), dtype=tf.string, name='sentence_input')\n",
    "    \n",
    "    # tokenizing the sentence\n",
    "    x = sentence_tokenizer(sentence_input)\n",
    "    \n",
    "    # creating embeddings for each token in sentence\n",
    "    embeddings = Embedding(\n",
    "        input_dim = NUM_WORDS,\n",
    "        output_dim = EMBEDDING_DIMS,\n",
    "        mask_zero = True,\n",
    "        name = 'word_embedding'\n",
    "    )(x)\n",
    "    \n",
    "    # stacking two bidirectional LSTMs\n",
    "    output_sequence = Bidirectional(LSTM(40, return_sequences=True), name='lstm_1')(embeddings)\n",
    "    output_sequence = Bidirectional(LSTM(40, return_sequences=True), name='lstm_2')(output_sequence)\n",
    "    \n",
    "    # passing the sequence through dense layer to compress the information\n",
    "    dense_sequence = TimeDistributed(Dense(25, activation='relu'), name='dense')(output_sequence)\n",
    "    \n",
    "    # passing the dense sequences through crf layer\n",
    "    predicted_sequence, potentials, sequence_length, crf_kernel = CRF(NUM_ENTITIES, name='crf')(dense_sequence)\n",
    "    \n",
    "    # define the train model\n",
    "    training_model = Model(sentence_input, potentials)\n",
    "    \n",
    "    # compile the model\n",
    "    training_model.compile(\n",
    "        loss=SigmoidFocalCrossEntropy(),\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    # create an inference model\n",
    "    inference_model = Model(sentence_input, predicted_sequence)\n",
    "    \n",
    "    return training_model, inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the traing and inferencing model\n",
    "lstm_training_model, lstm_inference_model = build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sentence_input (InputLayer  [(None, 1)]               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " sentence_tokenizer (TextVe  (None, 45)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " word_embedding (Embedding)  (None, 45, 50)            529400    \n",
      "                                                                 \n",
      " lstm_1 (Bidirectional)      (None, 45, 80)            29120     \n",
      "                                                                 \n",
      " lstm_2 (Bidirectional)      (None, 45, 80)            38720     \n",
      "                                                                 \n",
      " dense (TimeDistributed)     (None, 45, 25)            2025      \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 45),              1029      \n",
      "                              (None, 45, 21),                    \n",
      "                              (None,),                           \n",
      "                              (21, 21)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600294 (2.29 MB)\n",
      "Trainable params: 600294 (2.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the entity data\n",
    "\n",
    "# tokenizing the entities\n",
    "entity_tokenizer = TextVectorization(max_tokens=NUM_ENTITIES, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='entity_tokenizer')\n",
    "entity_tokenizer.adapt(ytrain)\n",
    "ytrain_tokenized = entity_tokenizer(ytrain)\n",
    "ytest_tokenized = entity_tokenizer(ytest)\n",
    "\n",
    "# one hot encoding the entitiy tokens\n",
    "entity_ohe = Lambda(lambda x: tf.one_hot(x, NUM_ENTITIES))\n",
    "ytrain_tokenized = entity_ohe(ytrain_tokenized)\n",
    "ytest_tokenized = entity_ohe(ytest_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "mc = tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "    'models/lstm.h5',\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "es = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "callbacks=[mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "150/150 [==============================] - 30s 75ms/step - loss: 0.4948\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 0.1951\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 0.1004\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 8s 54ms/step - loss: 0.0582\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 0.0394\n"
     ]
    }
   ],
   "source": [
    "history = lstm_training_model.fit(xtrain, ytrain_tokenized, epochs=5, batch_size=16, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the best model\n",
    "lstm_training_model = tensorflow.keras.models.load_model('models/lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 8s 15ms/step - loss: 0.0728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07284174114465714"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_training_model.evaluate(xtest,ytest_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the validation data somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing the Bi-LSTM+CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new orleans mother s day parade shooting one of the people hurt was a 10yearold girl what the hell is wrong with people\n",
      "B-other I-other I-other I-other I-other I-other O O O O O O O O O O O O O O O O O\n",
      "119\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "print(xtest[0])\n",
    "print(ytest[0])\n",
    "print(len(xtest[0]))\n",
    "print(len(ytest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O', 'O']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest[[2]][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to infer prediction from a text in xtest randomly\n",
    "def infer_lstm():\n",
    "    idx = np.random.choice(range(len(xtest)),1,replace=False)\n",
    "    text = xtest[idx]\n",
    "    pred_labels = lstm_inference_model.predict(text)\n",
    "    act_labels = ytest[idx]\n",
    "    text_len = len(text[0].split())\n",
    "    \n",
    "    pred_labels = np.asarray(entity_tokenizer.get_vocabulary())[list(pred_labels[0])]\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'text':text[0].split(),\n",
    "            'actual_labels':act_labels[0].split(),\n",
    "            'predicted_labels':pred_labels[:text_len]\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abc3340</td>\n",
       "      <td>O</td>\n",
       "      <td>b-tvshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>police</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conducting</td>\n",
       "      <td>O</td>\n",
       "      <td>b-tvshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homicide</td>\n",
       "      <td>O</td>\n",
       "      <td>b-person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>investigation</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>b-person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>avenue</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>v</td>\n",
       "      <td>O</td>\n",
       "      <td>b-person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>after</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>shooting</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>saturday</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>night</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>httpstco0q94pq9qdb</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text actual_labels predicted_labels\n",
       "0                   rt             O                o\n",
       "1              abc3340             O         b-tvshow\n",
       "2               police             O                o\n",
       "3           conducting             O         b-tvshow\n",
       "4                    a             O                o\n",
       "5             homicide             O         b-person\n",
       "6        investigation             O                o\n",
       "7                   on             O         b-person\n",
       "8               avenue             O                o\n",
       "9                    v             O         b-person\n",
       "10               after             O                o\n",
       "11                   a             O                o\n",
       "12            shooting             O                o\n",
       "13            saturday             O                o\n",
       "14               night             O                o\n",
       "15  httpstco0q94pq9qdb             O                o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForTokenClassification\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data for bert\n",
    "\n",
    "def preprocess_bert(text,labels):\n",
    "    \n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        tokenized_words = []\n",
    "        tokenized_labels = []\n",
    "        words = text[i].split()\n",
    "        entities = labels[i].split()\n",
    "        sentence_len = len(words)\n",
    "        for j in range(sentence_len):\n",
    "            tokenized_word = bert_tokenizer.tokenize(words[j])\n",
    "            tokenized_words.extend(tokenized_word)\n",
    "            tokenized_label = [entities[j]]*len(tokenized_word)\n",
    "            tokenized_labels.extend(tokenized_label)\n",
    "        text_list.append(' '.join(tokenized_words))\n",
    "        label_list.append(' '.join(tokenized_labels))\n",
    "    return text_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_tokenized_sentences, train_bert_tokenized_labels = preprocess_bert(xtrain,ytrain)\n",
    "test_bert_tokenized_sentences, test_bert_tokenized_labels = preprocess_bert(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam ##mie ##lynn ##smo ##m t ##g ##10 ##7 ##8 ##1 they will be all done by sunday trust me wink\n",
      "O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(train_bert_tokenized_sentences[0])\n",
    "print(train_bert_tokenized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the vectorizer layer before modeling\n",
    "sentence_indexer_bert = TextVectorization(max_tokens=NUM_WORDS, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='sentence_indexer_bert')\n",
    "sentence_indexer_bert.adapt(train_bert_tokenized_sentences)\n",
    "train_bert_sentence_index = sentence_indexer_bert(train_bert_tokenized_sentences)\n",
    "test_bert_sentence_index = sentence_indexer_bert(test_bert_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_type_ids = np.zeros(shape=(len(train_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "test_token_type_ids = np.zeros(shape=(len(test_bert_tokenized_sentences),SENTENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attn_mask = np.zeros(shape=(len(train_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "test_attn_mask = np.zeros(shape=(len(test_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "for i in range(len(train_bert_tokenized_sentences)):\n",
    "    length = min(len(train_bert_tokenized_sentences[i].split()),SENTENCE_LENGTH)\n",
    "    train_attn_mask[i,:length] = 1\n",
    "for i in range(len(test_bert_tokenized_sentences)):\n",
    "    length = min(len(test_bert_tokenized_sentences[i].split()),SENTENCE_LENGTH)\n",
    "    test_attn_mask[i,:length] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the entity data\n",
    "\n",
    "# tokenizing the entities\n",
    "entity_indexer_bert = TextVectorization(max_tokens=NUM_ENTITIES, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='entity_tokenizer')\n",
    "entity_indexer_bert.adapt(ytrain)\n",
    "train_bert_label_index = entity_indexer_bert(train_bert_tokenized_labels)\n",
    "test_bert_label_index = entity_indexer_bert(test_bert_tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# build bert model\n",
    "encoder = TFBertForTokenClassification.from_pretrained('bert-base-uncased',name='bert_layer')\n",
    "def build_bert_model():\n",
    "\n",
    "    # getting inputs to the model\n",
    "    input_sentence_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "    input_token_type_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "    input_attn_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    # sending inputs through the bert model\n",
    "    embeddings = encoder(\n",
    "        input_ids = input_sentence_ids,\n",
    "        token_type_ids = input_token_type_ids,\n",
    "        attention_mask = input_attn_ids,\n",
    "    )[0]\n",
    "\n",
    "    # sending the context vectors through dense layer with linear activation (softmax will be applied during the loss calculation)\n",
    "    output_logits = Dense(NUM_ENTITIES,activation='linear')(embeddings)\n",
    "\n",
    "    # compiling the model\n",
    "    model = Model(\n",
    "        inputs = [input_sentence_ids, input_token_type_ids, input_attn_ids],\n",
    "        outputs = [output_logits]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer = Adam()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)       [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)       [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_29 (InputLayer)       [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " bert_layer (TFBertForToken  TFTokenClassifierOutput(lo   1088931   ['input_28[0][0]',            \n",
      " Classification)             ss=None, logits=(None, 45,   86         'input_30[0][0]',            \n",
      "                              2),                                    'input_29[0][0]']            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 45, 21)               63        ['bert_layer[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108893249 (415.39 MB)\n",
      "Trainable params: 108893249 (415.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model = build_bert_model()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "mc = tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "    'models/bert.h5',\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "es = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "callbacks=[mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 437s 3s/step - loss: 0.8941\n"
     ]
    }
   ],
   "source": [
    "# fitting the bert model\n",
    "history = bert_model.fit(\n",
    "    [train_bert_sentence_index, train_token_type_ids, train_attn_mask],\n",
    "    train_bert_label_index,\n",
    "    epochs=1,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the best model\n",
    "bert_model = tensorflow.keras.models.load_model('models/bert.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 168s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_model.predict([test_bert_sentence_index,test_token_type_ids,test_attn_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breaking dawn ##returns to vancouver on january 11th http ##bit ##ly ##db ##dm ##s ##8'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_joined_labels(tokenized_text,tokenized_labels):\n",
    "    joined_labels = []\n",
    "    begin = 0\n",
    "    end = 0\n",
    "\n",
    "    for i in range(len(tokenized_text.split())):\n",
    "        \n",
    "        if i==len(tokenized_text.split())-1:\n",
    "            if begin==end:\n",
    "                joined_labels.append(tokenized_labels.split()[i])\n",
    "            else:\n",
    "                label_segment = tokenized_labels.split()[begin:end+1]\n",
    "                added = None\n",
    "                for j in label_segment:\n",
    "                    if j!='o':\n",
    "                        added = j\n",
    "                        break\n",
    "                if added == None:\n",
    "                    added = 'o'\n",
    "                joined_labels.append(added)\n",
    "        \n",
    "        elif str(tokenized_text.split()[i+1]).startswith(\"##\"):\n",
    "            end = i+1\n",
    "        \n",
    "        else:\n",
    "            if begin==end:\n",
    "                begin = i+1\n",
    "                end = i+1\n",
    "                joined_labels.append(tokenized_labels.split()[i])\n",
    "            else:\n",
    "                label_segment = tokenized_labels.split()[begin:end+1]\n",
    "                added = None\n",
    "                for j in label_segment:\n",
    "                    if j!='o':\n",
    "                        added = j\n",
    "                        break\n",
    "                if added == None:\n",
    "                    added = 'o'\n",
    "                joined_labels.append(added)\n",
    "                begin = i+1\n",
    "                end = i+1\n",
    "    return ' '.join(joined_labels)\n",
    "\n",
    "# tokenized_labels\n",
    "'breaking dawn ##returns to vancouver on january 11th http ##bit ##ly ##db ##dm ##s ##8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to infer prediction from a text in xtest randomly\n",
    "def infer_bert():\n",
    "    idx = np.random.choice(range(len(test_bert_sentence_index)),1,replace=False)[0]\n",
    "\n",
    "    text = test_bert_tokenized_sentences[idx]\n",
    "    labels_act = test_bert_tokenized_labels[idx]\n",
    "    label_pred = np.argmax(predictions[idx], axis=-1)\n",
    "    text_len = min(len(text.split()),SENTENCE_LENGTH)\n",
    "    label_pred = np.asarray(entity_indexer_bert.get_vocabulary())[list(label_pred)]\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'text':text.split(),\n",
    "            'actual_labels':labels_act.split(),\n",
    "            'predicted_labels':label_pred[:text_len]\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##ife</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##elli</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##ke</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>im</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>single</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bc</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>didn</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>##t</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>forward</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>message</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7th</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>grade</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text actual_labels predicted_labels\n",
       "0        rt             O                o\n",
       "1       hey             O                o\n",
       "2     ##ife             O                o\n",
       "3    ##elli             O                o\n",
       "4      ##ke             O                o\n",
       "5        im             O                o\n",
       "6    single             O                o\n",
       "7        bc             O                o\n",
       "8         i             O                o\n",
       "9      didn             O                o\n",
       "10      ##t             O                o\n",
       "11  forward             O                o\n",
       "12     that             O                o\n",
       "13     text             O                o\n",
       "14  message             O                o\n",
       "15       in             O                o\n",
       "16      7th             O                o\n",
       "17    grade             O                o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(45,), dtype=int64, numpy=\n",
       "array([ 124, 5009, 1278,   17,   21, 2257, 1450,   73,   15,    2,  144,\n",
       "       1302,   45,    5,  240,  820, 1370,  423,   62,    2,  886,   20,\n",
       "        635,   34,  144,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0])>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bert_sentence_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the data objects into dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
