{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Named Entity Recognition Case Study\n",
    "\n",
    "### About\n",
    "Twitter is a microblogging and social networking service on which users post and interact with messages known as \"tweets\". Every second, on average, around 6,000 tweets are tweeted on Twitter, corresponding to over 350,000 tweets sent per minute, 500 million tweets per day.\n",
    "\n",
    "### Problem statement \n",
    "Twitter wants to automatically tag and analyze tweets for better understanding of the trends and topics without being dependent on the hashtags that the users use. Many users do not use hashtags or sometimes use wrong or mis-spelled tags, so they want to completely remove this problem and create a system of recognizing important content of the tweets.\n",
    "\n",
    "### Objective\n",
    "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities.\n",
    "We need to train models that will be able to identify the various named entities.\n",
    "\n",
    "### Data\n",
    "Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing in Text file format.\n",
    "The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path variables\n",
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "data_path = os.path.join(root_path,'data')\n",
    "train_data_path = os.path.join(data_path,'wnut 16.txt.conll')\n",
    "test_data_path = os.path.join(data_path,'wnut 16test.txt.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training file\n",
    "with open(train_data_path,'r') as f:\n",
    "    train_raw = f.read()\n",
    "with open(test_data_path,'r') as f:\n",
    "    test_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "def extract_ner_from_conll(conll_data):\n",
    "    # Split the data into sentences based on empty lines\n",
    "    sentences = [sentence.strip() for sentence in conll_data.strip().split('\\n\\n')]\n",
    "    ner_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = []\n",
    "        for token_entity in sentence.split('\\n'):\n",
    "            token, entity = token_entity.split('\\t')\n",
    "            tokenised_sentence.append((token,entity))\n",
    "        ner_data.append(tokenised_sentence)\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the raw files\n",
    "train_data = extract_ner_from_conll(train_raw)\n",
    "test_data = extract_ner_from_conll(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SammieLynnsMom', 'O'), ('@tg10781', 'O'), ('they', 'O'), ('will', 'O'), ('be', 'O'), ('all', 'O'), ('done', 'O'), ('by', 'O'), ('Sunday', 'O'), ('trust', 'O'), ('me', 'O'), ('*wink*', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# checking sentences after preprocessing\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data (including padding and OOV token) = 10588\n",
      "Maximum sentence length = 39\n",
      "Minimum sentence length = 1\n"
     ]
    }
   ],
   "source": [
    "# number of words in the vocabulary and lenght of sentences in the training data\n",
    "\n",
    "sentence_lenghts = list()\n",
    "word_set = set()\n",
    "for sentence in train_data:\n",
    "    sentence_lenghts.append(len(sentence))\n",
    "    for word in sentence:\n",
    "        word_set.add(word[0])\n",
    "        \n",
    "NUM_WORDS = len(word_set)+2 # +2 to include padding and out of vocabulary\n",
    "print(f\"Number of unique words in training data (including padding and OOV token) = {NUM_WORDS}\")\n",
    "print(f\"Maximum sentence length = {max(sentence_lenghts)}\")\n",
    "print(f\"Minimum sentence length = {min(sentence_lenghts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the max sentence length if 39, we will take a length of 45 in our model to incorporate for edge cases in inference\n",
    "SENTENCE_LENGTH = 45\n",
    "# we will keep the embedding dimensions to be 50 since the number of datapoints is small\n",
    "EMBEDDING_DIMS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities in training data = 21\n"
     ]
    }
   ],
   "source": [
    "# number of entities\n",
    "entity_set = set()\n",
    "for sentence in train_data:\n",
    "    for word in sentence:\n",
    "        entity_set.add(word[1])\n",
    "        \n",
    "NUM_ENTITIES = len(entity_set)\n",
    "print(f\"Number of unique entities in training data = {NUM_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to prepare the data to be fed into the model\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(\"[^A-Za-z0-9]+\",'',str(text).lower())\n",
    "\n",
    "def prepare_data(text_data):\n",
    "    \n",
    "    # initialize empty lists for sentences and entities\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in text_data:\n",
    "        \n",
    "        # initialize empty lists for sentence text and corresponding entities\n",
    "        word_list = []\n",
    "        entity_list = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            word = token[0]\n",
    "            entity = token[1]\n",
    "            if word in punctuations:\n",
    "                continue\n",
    "            else:\n",
    "                word_list.append(clean_text(word))\n",
    "                entity_list.append(entity)\n",
    "        \n",
    "        sentences.append(word_list)\n",
    "        entities.append(entity_list)\n",
    "    \n",
    "    # create a single string for each sentence and entity by joining elements with whitespace\n",
    "    sentences = np.array([' '.join(sentence) for sentence in sentences])\n",
    "    entities = np.array([' '.join(entity) for entity in entities])\n",
    "    \n",
    "    return (sentences,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the datapoints in train file is very low, we will merge the datasets and prepare our own train and test data\n",
    "data = []\n",
    "for i in train_data:\n",
    "    data.append(i)\n",
    "for i in test_data:\n",
    "    data.append(i)\n",
    "\n",
    "xdata, ydata = prepare_data(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion\n",
      "@SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*\n",
      "\n",
      "After conversion\n",
      "sammielynnsmom tg10781 they will be all done by sunday trust me wink\n",
      "\n",
      "Entities\n",
      "O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# checking sentences are conversion\n",
    "print(f\"Before conversion\\n{' '.join([word[0] for word in data[0]])}\\n\")\n",
    "print(f\"After conversion\\n{xdata[0]}\\n\")\n",
    "print(f\"Entities\\n{ydata[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adapt the vectorizer layer before modeling\n",
    "sentence_tokenizer = TextVectorization(max_tokens=NUM_WORDS, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='sentence_tokenizer')\n",
    "sentence_tokenizer.adapt(xtrain)\n",
    "\n",
    "# indexing the tokens in train and test\n",
    "train_lstm_sentence_indexed = sentence_tokenizer(xtrain)\n",
    "test_lstm_sentence_indexed = sentence_tokenizer(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the embedding vector\n",
    "import gensim.downloader as api\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "embedding_matrix = np.zeros(shape=(NUM_WORDS,EMBEDDING_DIMS), dtype=np.float32)\n",
    "\n",
    "for i,word in enumerate(sentence_tokenizer.get_vocabulary()):\n",
    "    try:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build and compile model\n",
    "def build_lstm_model(name='bi-lstm+crf'):\n",
    "    \n",
    "    # input layer for getting sentences\n",
    "    sentence_input = Input(shape=(SENTENCE_LENGTH,), dtype=tf.float32, name='sentence_input')\n",
    "    \n",
    "    # creating embeddings for each token in sentence\n",
    "    embeddings = Embedding(\n",
    "        input_dim = NUM_WORDS,\n",
    "        output_dim = EMBEDDING_DIMS,\n",
    "        mask_zero = True,\n",
    "        name = 'word_embedding',\n",
    "        embeddings_initializer = tf.keras.initializers.constant(embedding_matrix)\n",
    "    )(sentence_input)\n",
    "    \n",
    "    # stacking two bidirectional LSTMs\n",
    "    output_sequence = Bidirectional(LSTM(50, return_sequences=True), name='lstm_1')(embeddings)\n",
    "    output_sequence = Bidirectional(LSTM(50, return_sequences=True), name='lstm_2')(output_sequence)\n",
    "    \n",
    "    # passing the sequence through dense layer to compress the information\n",
    "    dense_sequence = TimeDistributed(Dense(25, activation='relu'), name='dense')(output_sequence)\n",
    "    \n",
    "    # passing the dense sequences through crf layer\n",
    "    predicted_sequence, potentials, sequence_length, crf_kernel = CRF(NUM_ENTITIES, name='crf')(dense_sequence)\n",
    "    \n",
    "    # define the train model\n",
    "    training_model = Model(sentence_input, potentials)\n",
    "    \n",
    "    # compile the model\n",
    "    training_model.compile(\n",
    "        loss=SigmoidFocalCrossEntropy(),\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    # create an inference model\n",
    "    inference_model = Model(sentence_input, predicted_sequence)\n",
    "    \n",
    "    return training_model, inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the traing and inferencing model\n",
    "lstm_training_model, lstm_inference_model = build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sentence_input (InputLayer  [(None, 45)]              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " word_embedding (Embedding)  (None, 45, 300)           3176400   \n",
      "                                                                 \n",
      " lstm_1 (Bidirectional)      (None, 45, 100)           140400    \n",
      "                                                                 \n",
      " lstm_2 (Bidirectional)      (None, 45, 100)           60400     \n",
      "                                                                 \n",
      " dense (TimeDistributed)     (None, 45, 25)            2525      \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 45),              1029      \n",
      "                              (None, 45, 21),                    \n",
      "                              (None,),                           \n",
      "                              (21, 21)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3380754 (12.90 MB)\n",
      "Trainable params: 3380754 (12.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the entity data\n",
    "\n",
    "# tokenizing the entities\n",
    "entity_tokenizer = TextVectorization(max_tokens=NUM_ENTITIES, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='entity_tokenizer')\n",
    "entity_tokenizer.adapt(ytrain)\n",
    "ytrain_tokenized = entity_tokenizer(ytrain)\n",
    "ytest_tokenized = entity_tokenizer(ytest)\n",
    "\n",
    "# one hot encoding the entitiy tokens\n",
    "entity_ohe = Lambda(lambda x: tf.one_hot(x, NUM_ENTITIES))\n",
    "ytrain_tokenized = entity_ohe(ytrain_tokenized)\n",
    "ytest_tokenized = entity_ohe(ytest_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(root_path,'models','lstm.ckpt'),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "callbacks=[mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "313/313 [==============================] - 36s 73ms/step - loss: 0.3820 - val_loss: 0.1575\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.0876 - val_loss: 0.0535\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.0367 - val_loss: 0.0436\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0269 - val_loss: 0.0449\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 19s 59ms/step - loss: 0.0235 - val_loss: 0.0645\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0216 - val_loss: 0.0738\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 17s 56ms/step - loss: 0.0206 - val_loss: 0.1044\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0188 - val_loss: 0.1294\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.0212 - val_loss: 0.1290\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.0188 - val_loss: 0.1456\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 18s 56ms/step - loss: 0.0180 - val_loss: 0.1766\n"
     ]
    }
   ],
   "source": [
    "# fitting the lstm+crf model\n",
    "history_lstm = lstm_training_model.fit(\n",
    "    train_lstm_sentence_indexed, ytrain_tokenized,\n",
    "    validation_data=(test_lstm_sentence_indexed, ytest_tokenized), \n",
    "    epochs=50, \n",
    "    batch_size=16, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x26c55d15e70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the best model\n",
    "lstm_training_model.load_weights(os.path.join(root_path,'models','lstm.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 12ms/step - loss: 0.0436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04360351338982582"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating on validation data\n",
    "lstm_training_model.evaluate(test_lstm_sentence_indexed,ytest_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing the Bi-LSTM+CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to infer prediction from a text in xtest randomly\n",
    "def infer_lstm():\n",
    "    idx = np.random.choice(range(len(xtest)),1,replace=False)\n",
    "    text = xtest[idx]\n",
    "    text_tokenized = sentence_tokenizer(text)\n",
    "    pred_labels = lstm_inference_model.predict(text_tokenized)\n",
    "    act_labels = ytest[idx]\n",
    "    text_len = min(len(text[0].split()),SENTENCE_LENGTH)\n",
    "    \n",
    "    pred_labels = np.asarray(entity_tokenizer.get_vocabulary())[list(pred_labels[0])]\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'text':text[0].split(),\n",
    "            'actual_labels':act_labels[0].split(),\n",
    "            'predicted_labels':pred_labels[:text_len]\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miriamsaying</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>may</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mga</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patama</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>talagang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>di</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>naman</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>para</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sayo</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sadyang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>assumera</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ka</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kaya</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inaangkin</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mo</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text actual_labels predicted_labels\n",
       "0             rt             O                o\n",
       "1   miriamsaying             O                o\n",
       "2            may             O                o\n",
       "3            mga             O                o\n",
       "4         patama             O                o\n",
       "5       talagang             O                o\n",
       "6             di             O                o\n",
       "7          naman             O                o\n",
       "8           para             O                o\n",
       "9           sayo             O                o\n",
       "10       sadyang             O                o\n",
       "11      assumera             O                o\n",
       "12            ka             O                o\n",
       "13          lang             O                o\n",
       "14          kaya             O                o\n",
       "15     inaangkin             O                o\n",
       "16            mo             O                o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer random text from the validation data\n",
    "infer_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import TFBertForTokenClassification\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the bert tokenizer (Sub word tokenizer)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data for bert\n",
    "\n",
    "def preprocess_bert(text,labels):\n",
    "    \n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        tokenized_words = []\n",
    "        tokenized_labels = []\n",
    "        words = text[i].split()\n",
    "        entities = labels[i].split()\n",
    "        sentence_len = len(words)\n",
    "        for j in range(sentence_len):\n",
    "            tokenized_word = bert_tokenizer.tokenize(words[j])\n",
    "            tokenized_words.extend(tokenized_word)\n",
    "            tokenized_label = [entities[j]]*len(tokenized_word)\n",
    "            tokenized_labels.extend(tokenized_label)\n",
    "        text_list.append(' '.join(tokenized_words))\n",
    "        label_list.append(' '.join(tokenized_labels))\n",
    "    return text_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocessing to the sentences\n",
    "train_bert_tokenized_sentences, train_bert_tokenized_labels = preprocess_bert(xtrain,ytrain)\n",
    "test_bert_tokenized_sentences, test_bert_tokenized_labels = preprocess_bert(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new tee ##shi ##rts ordered with all new designs i hope people will like october 9th 2010 remington s annual october ##fest\n",
      "O O O O O O O O O O O O O O O O O O O B-other I-other I-other\n"
     ]
    }
   ],
   "source": [
    "# checking the sentences after tokenization\n",
    "print(train_bert_tokenized_sentences[0])\n",
    "print(train_bert_tokenized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the vectorizer layer before modeling\n",
    "sentence_indexer_bert = TextVectorization(max_tokens=NUM_WORDS, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='sentence_indexer_bert')\n",
    "sentence_indexer_bert.adapt(train_bert_tokenized_sentences)\n",
    "train_bert_sentence_index = sentence_indexer_bert(train_bert_tokenized_sentences)\n",
    "test_bert_sentence_index = sentence_indexer_bert(test_bert_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the token type ids for train and test sentences\n",
    "train_token_type_ids = np.zeros(shape=(len(train_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "test_token_type_ids = np.zeros(shape=(len(test_bert_tokenized_sentences),SENTENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiazing the attention masks for train and test sentences\n",
    "train_attn_mask = np.zeros(shape=(len(train_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "test_attn_mask = np.zeros(shape=(len(test_bert_tokenized_sentences),SENTENCE_LENGTH))\n",
    "for i in range(len(train_bert_tokenized_sentences)):\n",
    "    length = min(len(train_bert_tokenized_sentences[i].split()),SENTENCE_LENGTH)\n",
    "    train_attn_mask[i,:length] = 1\n",
    "for i in range(len(test_bert_tokenized_sentences)):\n",
    "    length = min(len(test_bert_tokenized_sentences[i].split()),SENTENCE_LENGTH)\n",
    "    test_attn_mask[i,:length] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the entity data\n",
    "\n",
    "# tokenizing the entities\n",
    "entity_indexer_bert = TextVectorization(max_tokens=NUM_ENTITIES, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='entity_tokenizer')\n",
    "entity_indexer_bert.adapt(ytrain)\n",
    "train_bert_label_index = entity_indexer_bert(train_bert_tokenized_labels)\n",
    "test_bert_label_index = entity_indexer_bert(test_bert_tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for modeling\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# build bert model\n",
    "encoder = TFBertForTokenClassification.from_pretrained('bert-base-uncased',name='bert_layer')\n",
    "def build_bert_model():\n",
    "\n",
    "    # getting inputs to the model\n",
    "    input_sentence_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "    input_token_type_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "    input_attn_ids = Input(shape=(SENTENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    # sending inputs through the bert model\n",
    "    embeddings = encoder(\n",
    "        input_ids = input_sentence_ids,\n",
    "        token_type_ids = input_token_type_ids,\n",
    "        attention_mask = input_attn_ids,\n",
    "    )[0]\n",
    "\n",
    "    # sending the context vectors through dense layer with linear activation (softmax will be applied during the loss calculation)\n",
    "    output_logits = Dense(NUM_ENTITIES,activation='linear')(embeddings)\n",
    "\n",
    "    # defining the model\n",
    "    model = Model(\n",
    "        inputs = [input_sentence_ids, input_token_type_ids, input_attn_ids],\n",
    "        outputs = [output_logits]\n",
    "    )\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer = Adam(),\n",
    "        run_eagerly=True\n",
    "    )\n",
    "\n",
    "    # returning the built model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " bert_layer (TFBertForToken  TFTokenClassifierOutput(lo   1088931   ['input_4[0][0]',             \n",
      " Classification)             ss=None, logits=(None, 45,   86         'input_6[0][0]',             \n",
      "                              2),                                    'input_5[0][0]']             \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 45, 21)               63        ['bert_layer[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108893249 (415.39 MB)\n",
      "Trainable params: 108893249 (415.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# calling the build model function and checking it's structure\n",
    "bert_model = build_bert_model()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(root_path,'models','bert.ckpt'),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "callbacks=[mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.9673INFO:tensorflow:Assets written to: c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\models\\bert.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\models\\bert.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1137s 4s/step - loss: 0.9673 - val_loss: 0.9566\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.9585INFO:tensorflow:Assets written to: c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\models\\bert.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\models\\bert.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1240s 4s/step - loss: 0.9585 - val_loss: 0.9535\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 1303s 4s/step - loss: 0.9572 - val_loss: 0.9539\n"
     ]
    }
   ],
   "source": [
    "# fitting the bert model\n",
    "history_bert = bert_model.fit(\n",
    "    [train_bert_sentence_index, train_token_type_ids, train_attn_mask], train_bert_label_index,\n",
    "    validation_data = ([test_bert_sentence_index, test_token_type_ids, test_attn_mask], test_bert_label_index),\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x26cbe72a650>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the best model\n",
    "bert_model.load_weights(os.path.join(root_path,'models','bert.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 56s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# getting the predictions\n",
    "predictions = bert_model.predict([test_bert_sentence_index,test_token_type_ids,test_attn_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to join back the tokenized text\n",
    "def get_joined_labels(tokenized_text,tokenized_labels):\n",
    "    joined_labels = []\n",
    "    begin = 0\n",
    "    end = 0\n",
    "\n",
    "    for i in range(len(tokenized_text.split())):\n",
    "        \n",
    "        if i==len(tokenized_text.split())-1:\n",
    "            if begin==end:\n",
    "                joined_labels.append(tokenized_labels.split()[i])\n",
    "            else:\n",
    "                label_segment = tokenized_labels.split()[begin:end+1]\n",
    "                added = None\n",
    "                for j in label_segment:\n",
    "                    if j!='o':\n",
    "                        added = j\n",
    "                        break\n",
    "                if added == None:\n",
    "                    added = 'o'\n",
    "                joined_labels.append(added)\n",
    "        \n",
    "        elif str(tokenized_text.split()[i+1]).startswith(\"##\"):\n",
    "            end = i+1\n",
    "        \n",
    "        else:\n",
    "            if begin==end:\n",
    "                begin = i+1\n",
    "                end = i+1\n",
    "                joined_labels.append(tokenized_labels.split()[i])\n",
    "            else:\n",
    "                label_segment = tokenized_labels.split()[begin:end+1]\n",
    "                added = None\n",
    "                for j in label_segment:\n",
    "                    if j!='o':\n",
    "                        added = j\n",
    "                        break\n",
    "                if added == None:\n",
    "                    added = 'o'\n",
    "                joined_labels.append(added)\n",
    "                begin = i+1\n",
    "                end = i+1\n",
    "    return ' '.join(joined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining back the tokenized text\n",
    "pred_labels = np.argmax(predictions, axis=-1)\n",
    "bert_joined_labels = []\n",
    "for i in range(len(xtest)):\n",
    "    text = test_bert_tokenized_sentences[i]\n",
    "    lbls = ' '.join(np.asarray(entity_indexer_bert.get_vocabulary())[list(pred_labels[i])])\n",
    "    bert_joined_labels.append(get_joined_labels(text,lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to infer prediction from a text in xtest randomly\n",
    "def infer_bert():\n",
    "    idx = np.random.choice(range(len(test_bert_sentence_index)),1,replace=False)[0]\n",
    "\n",
    "    text = xtest[idx]\n",
    "    labels_act = ytest[idx]\n",
    "    pred_labels = bert_joined_labels[idx]\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'text':text.split(),\n",
    "            'actual_labels':labels_act.split(),\n",
    "            'predicted_labels':pred_labels.split()\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miriamsaying</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>may</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mga</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patama</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>talagang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>di</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>naman</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>para</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sayo</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sadyang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>assumera</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ka</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lang</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kaya</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inaangkin</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mo</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text actual_labels predicted_labels\n",
       "0             rt             O                o\n",
       "1   miriamsaying             O                o\n",
       "2            may             O                o\n",
       "3            mga             O                o\n",
       "4         patama             O                o\n",
       "5       talagang             O                o\n",
       "6             di             O                o\n",
       "7          naman             O                o\n",
       "8           para             O                o\n",
       "9           sayo             O                o\n",
       "10       sadyang             O                o\n",
       "11      assumera             O                o\n",
       "12            ka             O                o\n",
       "13          lang             O                o\n",
       "14          kaya             O                o\n",
       "15     inaangkin             O                o\n",
       "16            mo             O                o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer random text from the validation data\n",
    "infer_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- The BI-LSTM+CRF model tends to overfit to the data\n",
    "- The prediction from the CRF layer is not accurate\n",
    "- Since BERT is larger model with model weights and more layers, it takes a lot of time to train\n",
    "- The loss in BERT is larger compared to that in LSTM\n",
    "- BERT tends to perform better than the LSRM model even though the loss is high\n",
    "- Early stoping and Model checkpoint callbacks assists in training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q**. Defining the problem statements and where can this and modifications of this be used?<br>\n",
    "**Ans**. Similar model can be used in Part-of-Speech tagging (POS tagging) or any other problem where we need a prediction at a token level. Meaning that the input length and the output length is the same.\n",
    "\n",
    "**Q**. Explain the data format (conll bio format)<br>\n",
    "**Ans**. Conll format is a text storing format where each word in the text is separated by a line (\\n), and annotation of that word is separated by tab (\\t) and each text is separated by two lines (\\n\\n).\n",
    "\n",
    "**Q**. What other ner data annotation formats are available and how are they different<br>\n",
    "**Ans**. Other formats can be BIO, IOB, JSON and XML\n",
    "\n",
    "**Q**. Why do we need tokenization of the data in our case<br>\n",
    "**Ans**. We need tokenization in our case to divide the sentence into smaller substituents in order to capture the sequence, context and attention mechanism. If we do not tokenize our text, the input to the model will be one big string. Another problem will be to convert the string to vectors.\n",
    "\n",
    "**Q**. What other models can you use for this task<br>\n",
    "**Ans**. We can replace LSTM with RNN or GRU and couple them with CRF layer. Or in case of BERT, we can use transformers (encoder-decoder architecture) or GPT architecture.\n",
    "\n",
    "**Q**. Did early stopping have any effect on the training and results.<br>\n",
    "**Ans**. Yes. Since the model started to overfit as the epochs progresses, early stopping caused the training to stop since the performance was not improving.\n",
    "\n",
    "**Q**. How does the BERT model expect a pair of sentences to be processed?<br>\n",
    "**Ans**. The BERT model expects a token_type_id tensor, which represents which sentence does each of the token belongs to. The pair of sentences are merged into a single vector and token_type_id is used to distinguish between them.\n",
    "\n",
    "**Q**. Why choose Attention based models over Recurrent based ones?<br>\n",
    "**Ans**. Recurrent based models have problems capturing long term dependencies of the words. Also, each word can be connected to multiple other words which can be capture by the attention heads. Hence, attention based models are preferred if long term dependencies or connection of words is required.\n",
    "\n",
    "**Q**. Differentiate BERT and simple transformers<br>\n",
    "**Ans**. BERT can be considered as the encoder only part of the transformers. In transformer architecture, first the input is passed through the encoder, then through the decoder and then the final prediction is made. But in BERT, the encoder only architecture is responsible for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
