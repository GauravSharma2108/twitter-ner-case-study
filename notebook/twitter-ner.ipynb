{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Named Entity Recognition Case Study\n",
    "\n",
    "### About\n",
    "Twitter is a microblogging and social networking service on which users post and interact with messages known as \"tweets\". Every second, on average, around 6,000 tweets are tweeted on Twitter, corresponding to over 350,000 tweets sent per minute, 500 million tweets per day.\n",
    "\n",
    "### Problem statement \n",
    "Twitter wants to automatically tag and analyze tweets for better understanding of the trends and topics without being dependent on the hashtags that the users use. Many users do not use hashtags or sometimes use wrong or mis-spelled tags, so they want to completely remove this problem and create a system of recognizing important content of the tweets.\n",
    "\n",
    "### Objective\n",
    "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities.\n",
    "We need to train models that will be able to identify the various named entities.\n",
    "\n",
    "### Data\n",
    "Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing in Text file format.\n",
    "The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "data_path = os.path.join(root_path,'data')\n",
    "train_data_path = os.path.join(data_path,'wnut 16.txt.conll')\n",
    "test_data_path = os.path.join(data_path,'wnut 16test.txt.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training file\n",
    "with open(train_data_path,'r') as f:\n",
    "    train_raw = f.read()\n",
    "with open(test_data_path,'r') as f:\n",
    "    test_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "def extract_ner_from_conll(conll_data):\n",
    "    # Split the data into sentences based on empty lines\n",
    "    sentences = [sentence.strip() for sentence in conll_data.strip().split('\\n\\n')]\n",
    "    ner_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = []\n",
    "        for token_entity in sentence.split('\\n'):\n",
    "            token, entity = token_entity.split('\\t')\n",
    "            tokenised_sentence.append((token,entity))\n",
    "        ner_data.append(tokenised_sentence)\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the raw files\n",
    "train_data = extract_ner_from_conll(train_raw)\n",
    "test_data = extract_ner_from_conll(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SammieLynnsMom', 'O'), ('@tg10781', 'O'), ('they', 'O'), ('will', 'O'), ('be', 'O'), ('all', 'O'), ('done', 'O'), ('by', 'O'), ('Sunday', 'O'), ('trust', 'O'), ('me', 'O'), ('*wink*', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# checking sentences after preprocessing\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data (including padding and OOV token) = 10588\n",
      "Maximum sentence length = 39\n",
      "Minimum sentence length = 1\n"
     ]
    }
   ],
   "source": [
    "# number of words in the vocabulary and lenght of sentences in the training data\n",
    "\n",
    "sentence_lenghts = list()\n",
    "word_set = set()\n",
    "for sentence in train_data:\n",
    "    sentence_lenghts.append(len(sentence))\n",
    "    for word in sentence:\n",
    "        word_set.add(word[0])\n",
    "        \n",
    "NUM_WORDS = len(word_set)+2 # +2 to include padding and out of vocabulary\n",
    "print(f\"Number of unique words in training data (including padding and OOV token) = {NUM_WORDS}\")\n",
    "print(f\"Maximum sentence length = {max(sentence_lenghts)}\")\n",
    "print(f\"Minimum sentence length = {min(sentence_lenghts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the max sentence length if 39, we will take a length of 45 in our model to incorporate for edge cases in inference\n",
    "SENTENCE_LENGTH = 45\n",
    "# we will keep the embedding dimensions to be 50 since the number of datapoints is small\n",
    "EMBEDDING_DIMS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities in training data = 21\n"
     ]
    }
   ],
   "source": [
    "# number of entities\n",
    "entity_set = set()\n",
    "for sentence in train_data:\n",
    "    for word in sentence:\n",
    "        entity_set.add(word[1])\n",
    "        \n",
    "NUM_ENTITIES = len(entity_set)\n",
    "print(f\"Number of unique entities in training data = {NUM_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to prepare the data to be fed into the model\n",
    "# --> punctuation should be handled along with their entities\n",
    "def prepare_data(text_data):\n",
    "    \n",
    "    # initialize empty lists for sentences and entities\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in text_data:\n",
    "        \n",
    "        # initialize empty lists for sentence text and corresponding entities\n",
    "        word_list = []\n",
    "        entity_list = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            word_list.append(token[0])\n",
    "            entity_list.append(token[1])\n",
    "        \n",
    "        sentences.append(word_list)\n",
    "        entities.append(entity_list)\n",
    "    \n",
    "    # create a single string for each sentence and entity by joining elements with whitespace\n",
    "    sentences = np.array([' '.join(sentence) for sentence in sentences])\n",
    "    entities = np.array([' '.join(entity) for entity in entities])\n",
    "    \n",
    "    return (sentences,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the prepare_data function on the train and test data\n",
    "xtrain, ytrain = prepare_data(train_data)\n",
    "xtest, ytest = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*\n",
      "O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# checking sentences are conversion\n",
    "print(xtrain[0])\n",
    "print(ytrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Bidirectional, LSTM, TimeDistributed, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adapt the vectorizer layer before modeling\n",
    "sentence_tokenizer = TextVectorization(max_tokens=NUM_WORDS, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='sentence_tokenizer')\n",
    "sentence_tokenizer.adapt(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build and compile model\n",
    "def build_lstm_model(name='bi-lstm+crf'):\n",
    "    \n",
    "    # input layer for getting sentences\n",
    "    sentence_input = Input(shape=(1,), dtype=tf.string, name='sentence_input')\n",
    "    \n",
    "    # tokenizing the sentence\n",
    "    x = sentence_tokenizer(sentence_input)\n",
    "    \n",
    "    # creating embeddings for each token in sentence\n",
    "    embeddings = Embedding(\n",
    "        input_dim = NUM_WORDS,\n",
    "        output_dim = EMBEDDING_DIMS,\n",
    "        mask_zero = True,\n",
    "        name = 'word_embedding'\n",
    "    )(x)\n",
    "    \n",
    "    # stacking two bidirectional LSTMs\n",
    "    output_sequence = Bidirectional(LSTM(40, return_sequences=True), name='lstm_1')(embeddings)\n",
    "    output_sequence = Bidirectional(LSTM(40, return_sequences=True), name='lstm_2')(output_sequence)\n",
    "    \n",
    "    # passing the sequence through dense layer to compress the information\n",
    "    dense_sequence = TimeDistributed(Dense(25, activation='relu'), name='dense')(output_sequence)\n",
    "    \n",
    "    # passing the dense sequences through crf layer\n",
    "    predicted_sequence, potentials, sequence_length, crf_kernel = CRF(NUM_ENTITIES, name='crf')(dense_sequence)\n",
    "    \n",
    "    # define the train model\n",
    "    training_model = Model(sentence_input, potentials)\n",
    "    \n",
    "    # compile the model\n",
    "    training_model.compile(\n",
    "        loss=SigmoidFocalCrossEntropy(),\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    # create an inference model\n",
    "    inference_model = Model(sentence_input, predicted_sequence)\n",
    "    \n",
    "    return training_model, inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\psyki\\Downloads\\Learning\\github-repos\\twitter-ner-case-study\\venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the traing and inferencing model\n",
    "lstm_training_model, lstm_inference_model = build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sentence_input (InputLayer  [(None, 1)]               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " sentence_tokenizer (TextVe  (None, 45)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " word_embedding (Embedding)  (None, 45, 50)            529400    \n",
      "                                                                 \n",
      " lstm_1 (Bidirectional)      (None, 45, 80)            29120     \n",
      "                                                                 \n",
      " lstm_2 (Bidirectional)      (None, 45, 80)            38720     \n",
      "                                                                 \n",
      " dense (TimeDistributed)     (None, 45, 25)            2025      \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 45),              1029      \n",
      "                              (None, 45, 21),                    \n",
      "                              (None,),                           \n",
      "                              (21, 21)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600294 (2.29 MB)\n",
      "Trainable params: 600294 (2.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the entity data\n",
    "\n",
    "# tokenizing the entities\n",
    "entity_tokenizer = TextVectorization(max_tokens=NUM_ENTITIES, output_sequence_length=SENTENCE_LENGTH, standardize='lower', name='entity_tokenizer')\n",
    "entity_tokenizer.adapt(ytrain)\n",
    "ytrain_tokenized = entity_tokenizer(ytrain)\n",
    "ytest_tokenized = entity_tokenizer(ytest)\n",
    "\n",
    "# one hot encoding the entitiy tokens\n",
    "entity_ohe = Lambda(lambda x: tf.one_hot(x, NUM_ENTITIES))\n",
    "ytrain_tokenized = entity_ohe(ytrain_tokenized)\n",
    "ytest_tokenized = entity_ohe(ytest_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.1776\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0876\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0466\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0281\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0191\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0137\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0110\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0087\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0072\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0062\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0052\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0043\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0037\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0037\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0033\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0022\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0019\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0029\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0017\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "history = lstm_training_model.fit(xtrain, ytrain_tokenized, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 1s 12ms/step - loss: 0.5763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5762874484062195"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_training_model.evaluate(xtest,ytest_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the validation data somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing the Bi-LSTM+CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Orleans Mother 's Day Parade shooting . One of the people hurt was a 10-year-old girl . WHAT THE HELL IS WRONG WITH PEOPLE ?\n",
      "B-other I-other I-other I-other I-other I-other O O O O O O O O O O O O O O O O O O O O\n",
      "128\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "print(xtest[0])\n",
    "print(ytest[0])\n",
    "print(len(xtest[0]))\n",
    "print(len(ytest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-movie', 'O', 'O']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest[[2]][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to infer prediction from a text in xtest randomly\n",
    "def infer_lstm():\n",
    "    idx = np.random.choice(range(len(xtest)),1,replace=False)\n",
    "    text = xtest[idx]\n",
    "    pred_labels = lstm_inference_model.predict(text)\n",
    "    act_labels = ytest[idx]\n",
    "    text_len = len(text[0].split())\n",
    "    \n",
    "    pred_labels = np.asarray(entity_tokenizer.get_vocabulary())[list(pred_labels[0])]\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'text':text[0].split(),\n",
    "            'actual_labels':act_labels[0].split(),\n",
    "            'predicted_labels':pred_labels[:text_len]\n",
    "        }\n",
    "    )\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gunman</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>identified</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shooting</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deaths</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Marines</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>B-geo-loc</td>\n",
       "      <td>b-other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Navy</td>\n",
       "      <td>B-other</td>\n",
       "      <td>b-company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>facility</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://www.foxnews.com/us/2015/07/16/shooting-...</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>…</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>via</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@FoxNews</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Android</td>\n",
       "      <td>B-product</td>\n",
       "      <td>b-product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>app</td>\n",
       "      <td>O</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text actual_labels  \\\n",
       "0                                              Gunman             O   \n",
       "1                                          identified             O   \n",
       "2                                                  in             O   \n",
       "3                                            shooting             O   \n",
       "4                                              deaths             O   \n",
       "5                                                  of             O   \n",
       "6                                                   4             O   \n",
       "7                                             Marines             O   \n",
       "8                                                  at             O   \n",
       "9                                           Tennessee     B-geo-loc   \n",
       "10                                               Navy       B-other   \n",
       "11                                           facility             O   \n",
       "12  http://www.foxnews.com/us/2015/07/16/shooting-...             O   \n",
       "13                                                  …             O   \n",
       "14                                                via             O   \n",
       "15                                                the             O   \n",
       "16                                           @FoxNews             O   \n",
       "17                                            Android     B-product   \n",
       "18                                                app             O   \n",
       "\n",
       "   predicted_labels  \n",
       "0                 o  \n",
       "1                 o  \n",
       "2                 o  \n",
       "3                 o  \n",
       "4                 o  \n",
       "5                 o  \n",
       "6                 o  \n",
       "7                 o  \n",
       "8                 o  \n",
       "9           b-other  \n",
       "10        b-company  \n",
       "11                o  \n",
       "12                o  \n",
       "13                o  \n",
       "14                o  \n",
       "15                o  \n",
       "16                o  \n",
       "17        b-product  \n",
       "18                o  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
