{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Named Entity Recognition Case Study\n",
    "\n",
    "### About\n",
    "Twitter is a microblogging and social networking service on which users post and interact with messages known as \"tweets\". Every second, on average, around 6,000 tweets are tweeted on Twitter, corresponding to over 350,000 tweets sent per minute, 500 million tweets per day.\n",
    "\n",
    "### Problem statement \n",
    "Twitter wants to automatically tag and analyze tweets for better understanding of the trends and topics without being dependent on the hashtags that the users use. Many users do not use hashtags or sometimes use wrong or mis-spelled tags, so they want to completely remove this problem and create a system of recognizing important content of the tweets.\n",
    "\n",
    "### Objective\n",
    "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities.\n",
    "We need to train models that will be able to identify the various named entities.\n",
    "\n",
    "### Data\n",
    "Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing in Text file format.\n",
    "The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "data_path = os.path.join(root_path,'data')\n",
    "train_data_path = os.path.join(data_path,'wnut 16.txt.conll')\n",
    "test_data_path = os.path.join(data_path,'wnut 16test.txt.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training file\n",
    "with open(train_data_path,'r') as f:\n",
    "    train_raw = f.read()\n",
    "with open(test_data_path,'r') as f:\n",
    "    test_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "def extract_ner_from_conll(conll_data):\n",
    "    # Split the data into sentences based on empty lines\n",
    "    sentences = [sentence.strip() for sentence in conll_data.strip().split('\\n\\n')]\n",
    "    ner_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = []\n",
    "        for token_entity in sentence.split('\\n'):\n",
    "            token, entity = token_entity.split('\\t')\n",
    "            tokenised_sentence.append((token,entity))\n",
    "        ner_data.append(tokenised_sentence)\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the raw files\n",
    "train_data = extract_ner_from_conll(train_raw)\n",
    "test_data = extract_ner_from_conll(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SammieLynnsMom', 'O'), ('@tg10781', 'O'), ('they', 'O'), ('will', 'O'), ('be', 'O'), ('all', 'O'), ('done', 'O'), ('by', 'O'), ('Sunday', 'O'), ('trust', 'O'), ('me', 'O'), ('*wink*', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# checking sentences after preprocessing\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data (including padding and OOV token) = 10588\n",
      "Maximum sentence length = 39\n",
      "Minimum sentence length = 1\n"
     ]
    }
   ],
   "source": [
    "# number of words in the vocabulary and lenght of sentences in the training data\n",
    "\n",
    "sentence_lenghts = list()\n",
    "word_set = set()\n",
    "for sentence in train_data:\n",
    "    sentence_lenghts.append(len(sentence))\n",
    "    for word in sentence:\n",
    "        word_set.add(word[0])\n",
    "        \n",
    "NUM_WORDS = len(word_set)+2 # +2 to include padding and out of vocabulary\n",
    "print(f\"Number of unique words in training data (including padding and OOV token) = {NUM_WORDS}\")\n",
    "print(f\"Maximum sentence length = {max(sentence_lenghts)}\")\n",
    "print(f\"Minimum sentence length = {min(sentence_lenghts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the max sentence length if 39, we will take a length of 45 in our model to incorporate for edge cases in inference\n",
    "SENTENCE_LENGTH = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entities in training data = 21\n"
     ]
    }
   ],
   "source": [
    "# number of entities\n",
    "entity_set = set()\n",
    "for sentence in train_data:\n",
    "    for word in sentence:\n",
    "        entity_set.add(word[1])\n",
    "        \n",
    "NUM_ENTITIES = len(entity_set)\n",
    "print(f\"Number of unique entities in training data = {NUM_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the number of entities are 21, we can define it as number of all possible entities\n",
    "num_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to prepare the data to be fed into the model\n",
    "def prepare_data(text_data):\n",
    "    \n",
    "    # initialize empty lists for sentences and entities\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in text_data:\n",
    "        \n",
    "        # initialize empty lists for sentence text and corresponding entities\n",
    "        word_list = []\n",
    "        entity_list = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            word_list.append(token[0])\n",
    "            entity_list.append(token[1])\n",
    "        \n",
    "        sentences.append(word_list)\n",
    "        entities.append(entity_list)\n",
    "    \n",
    "    # create a single string for each sentence and entity by joining elements with whitespace\n",
    "    sentences = [' '.join(sentence) for sentence in sentences]\n",
    "    entities = [' '.join(entity) for entity in entities]\n",
    "    \n",
    "    return (sentences,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the prepare_data function on the train and test data\n",
    "xtrain, ytrain = prepare_data(train_data)\n",
    "xtest, ytest = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*\n",
      "O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# checking sentences are conversion\n",
    "print(xtrain[0])\n",
    "print(ytrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try keras text vectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "temp_layer = TextVectorization(max_tokens=num_words+2, output_sequence_length=40)\n",
    "temp_layer.adapt(xtrain[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 40), dtype=int64, numpy=\n",
       "array([[ 8,  6,  5,  3, 12, 13, 10, 11,  7,  4,  9,  2,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 12,  1,  1,  1,  1,  1,\n",
       "         1,  1, 12,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_layer(xtrain[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
