{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Named Entity Recognition Case Study\n",
    "\n",
    "### About\n",
    "Twitter is a microblogging and social networking service on which users post and interact with messages known as \"tweets\". Every second, on average, around 6,000 tweets are tweeted on Twitter, corresponding to over 350,000 tweets sent per minute, 500 million tweets per day.\n",
    "\n",
    "### Problem statement \n",
    "Twitter wants to automatically tag and analyze tweets for better understanding of the trends and topics without being dependent on the hashtags that the users use. Many users do not use hashtags or sometimes use wrong or mis-spelled tags, so they want to completely remove this problem and create a system of recognizing important content of the tweets.\n",
    "\n",
    "### Objective\n",
    "Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities.\n",
    "We need to train models that will be able to identify the various named entities.\n",
    "\n",
    "### Data\n",
    "Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing in Text file format.\n",
    "The CoNLL format is a text file with one word per line with sentences separated by an empty line. The first word in a line should be the word and the last word should be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "data_path = os.path.join(root_path,'data')\n",
    "train_data_path = os.path.join(data_path,'wnut 16.txt.conll')\n",
    "test_data_path = os.path.join(data_path,'wnut 16test.txt.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the training file\n",
    "with open(train_data_path,'r') as f:\n",
    "    train_raw = f.read()\n",
    "with open(test_data_path,'r') as f:\n",
    "    test_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "def extract_ner_from_conll(conll_data):\n",
    "    # Split the data into sentences based on empty lines\n",
    "    sentences = [sentence.strip() for sentence in conll_data.strip().split('\\n\\n')]\n",
    "    ner_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = []\n",
    "        for token_entity in sentence.split('\\n'):\n",
    "            token, entity = token_entity.split('\\t')\n",
    "            tokenised_sentence.append((token,entity))\n",
    "        ner_data.append(tokenised_sentence)\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the raw files\n",
    "train_data = extract_ner_from_conll(train_raw)\n",
    "test_data = extract_ner_from_conll(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@SammieLynnsMom', 'O'), ('@tg10781', 'O'), ('they', 'O'), ('will', 'O'), ('be', 'O'), ('all', 'O'), ('done', 'O'), ('by', 'O'), ('Sunday', 'O'), ('trust', 'O'), ('me', 'O'), ('*wink*', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# checking sentences after preprocessing\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to prepare the data to be fed into the model\n",
    "def prepare_data(text_data):\n",
    "    \n",
    "    # initialize empty lists for sentences and entities\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in text_data:\n",
    "        \n",
    "        # initialize empty lists for sentence text and corresponding entities\n",
    "        word_list = []\n",
    "        entity_list = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            word_list.append(token[0])\n",
    "            entity_list.append(token[1])\n",
    "        \n",
    "        sentences.append(word_list)\n",
    "        entities.append(entity_list)\n",
    "    \n",
    "    return (sentences,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the prepare_data function on the train and test data\n",
    "xtrain, ytrain = prepare_data(train_data)\n",
    "xtest, ytest = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@SammieLynnsMom', '@tg10781', 'they', 'will', 'be', 'all', 'done', 'by', 'Sunday', 'trust', 'me', '*wink*']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# checking sentences are conversion\n",
    "print(xtrain[0])\n",
    "print(ytrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
